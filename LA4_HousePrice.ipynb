{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403A51L42/23CSBTB52/blob/main/LA4_HousePrice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g9NFM9cagLhr"
      },
      "outputs": [],
      "source": [
        "import numpy as np            # Importing the NumPy library for numerical operations\n",
        "import pandas as pd           # Importing the pandas library for data manipulation and analysis\n",
        "import matplotlib.pyplot as plt  # Importing matplotlib's pyplot for data visualization\n",
        "import seaborn as sns         # Importing seaborn for statistical data visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive       # Importing the drive module from Google Colab to interact with Google Drive\n",
        "drive.mount('/content/drive')        # Mounting Google Drive to the '/content/drive' path to access its files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsytk0rwgbRo",
        "outputId": "c0c21db5-283d-4611-9788-be5be86fe58e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/USA_Housing.csv'"
      ],
      "metadata": {
        "id": "7zQjoa8Sg0iH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(file_path)    # Reading a CSV file from the specified 'file_path' into a pandas DataFrame\n",
        "df.head()                      # Displaying the first 5 rows of the DataFrame to get a quick overview of the data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "xO3gcptYhHee",
        "outputId": "8d52cbc7-87be-4e8d-a00d-51b07f849605"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/USA_Housing.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-053ca39ac09f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Reading a CSV file from the specified 'file_path' into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                      \u001b[0;31m# Displaying the first 5 rows of the DataFrame to get a quick overview of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/USA_Housing.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info(verbose=True)    # Provides a summary of the DataFrame, including data types, non-null counts, and memory usage\n",
        "                         # 'verbose=True' ensures detailed output for each column\n"
      ],
      "metadata": {
        "id": "4OWz2SaNhN8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])  # Generates summary statistics for numeric columns\n",
        "                                                      # 'percentiles' specifies custom percentiles (10%, 25%, 50%, 75%, 90%)\n"
      ],
      "metadata": {
        "id": "GfUxn084hZwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns    # Returns the column names of the DataFrame as an Index object\n"
      ],
      "metadata": {
        "id": "NvgTmnUjhmWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df)    # Creates pairwise scatter plots for each numeric variable in the DataFrame using Seaborn\n",
        "                    # Helps in visualizing relationships between variables and identifying patterns or correlations\n"
      ],
      "metadata": {
        "id": "syO9u2BQh4b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Price'].plot.hist(bins=25, figsize=(8, 4))  # Creates a histogram of the 'Price' column with 25 bins\n",
        "                                                # The 'figsize' parameter adjusts the size of the plot to 8x4 inches\n"
      ],
      "metadata": {
        "id": "jSW8-KMyiIuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Price'].plot.density()  # Plots a kernel density estimate (KDE) for the 'Price' column\n",
        "                            # Provides a smoothed estimate of the distribution of the data\n"
      ],
      "metadata": {
        "id": "uINpruP7igZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop specific columns by name (e.g., 'Address')\n",
        "df_cleaned = df.drop(columns=['Address'])  # Replace 'Address' with the actual column name\n"
      ],
      "metadata": {
        "id": "CPl0VSGsiu4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now compute the correlation\n",
        "df_cleaned.corr()"
      ],
      "metadata": {
        "id": "KgagGrSEi6Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A heatmap is a data visualization technique** that displays data in a matrix format where individual values are represented by colors.\n",
        "\n",
        " This allows you to **easily visualize the magnitude** of values across a matrix or grid."
      ],
      "metadata": {
        "id": "iWOgBm95jcK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))                    # Creates a new figure with a size of 10x7 inches\n",
        "sns.heatmap(df_cleaned.corr(), annot=True, linewidths=2)  # Generates a heatmap of the correlation matrix for 'df_cleaned'\n",
        "                                                         # 'annot=True' displays the correlation coefficients in each cell\n",
        "                                                         # 'linewidths=2' adds a 2-pixel-wide line between cells in the heatmap\n"
      ],
      "metadata": {
        "id": "rPOg2VdHi9i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_column = list(df.columns)    # Converts the column names of the DataFrame into a list\n",
        "len_feature = len(l_column)   # Calculates the number of columns in the DataFrame (length of the column list)\n",
        "l_column                      # Displays the list of column names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "CYObGjGgjNM0",
        "outputId": "98ae81b2-1cdb-4327-9ba6-f43e23d9d0b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-356ccd79a9e8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Converts the column names of the DataFrame into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_column\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Calculates the number of columns in the DataFrame (length of the column list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ml_column\u001b[0m                      \u001b[0;31m# Displays the list of column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Put all the numerical features in X and Price in y,\n",
        "ignore Address which is string for linear regression**"
      ],
      "metadata": {
        "id": "iZF3x178j3LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[l_column[0:len_feature-2]]    # Selects all columns from the DataFrame except the last two columns for features (X)\n",
        "y = df[l_column[len_feature-2]]      # Selects the second-to-last column as the target variable (y)\n"
      ],
      "metadata": {
        "id": "svWLRqx7jv5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature set size:\", X.shape)    # Prints the dimensions of the feature set (X), showing the number of rows and columns\n",
        "print(\"Variable set size:\", y.shape)   # Prints the dimensions of the target variable (y), showing the number of rows (typically a single column)\n"
      ],
      "metadata": {
        "id": "_t8RDS7-kQ1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()    # Displays the first 5 rows of the feature set (X) to provide a quick overview\n"
      ],
      "metadata": {
        "id": "ja6EKVjgk3WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import train_test_split function from scikit-learn**"
      ],
      "metadata": {
        "id": "OGJlpOPLl4ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split  # Imports the train_test_split function from scikit-learn for splitting data into training and testing sets\n"
      ],
      "metadata": {
        "id": "tgXEpRUBlGCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create X and y train and test splits in one command using\n",
        "a split ratio and a random seed**"
      ],
      "metadata": {
        "id": "HWf2DX9pmJLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                            test_size=0.3, random_state=123)\n",
        "                            # Splits the feature set (X) and target variable (y) into training and testing sets\n",
        "                            # test_size=0.3 specifies that 30% of the data will be used for testing, and 70% for training\n",
        "                            # random_state=123 ensures reproducibility by setting a fixed seed for random number generation\n"
      ],
      "metadata": {
        "id": "BA70KhHJmB_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the size and shape of train/test splits (it should be in the ratio as per test_size parameter above)**"
      ],
      "metadata": {
        "id": "sd_wV2yUmmTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training feature set size:\", X_train.shape)    # Prints the dimensions of the training feature set (X_train)\n",
        "print(\"Test feature set size:\", X_test.shape)          # Prints the dimensions of the test feature set (X_test)\n",
        "print(\"Training variable set size:\", y_train.shape)    # Prints the dimensions of the training target variable (y_train)\n",
        "print(\"Test variable set size:\", y_test.shape)         # Prints the dimensions of the test target variable (y_test)\n"
      ],
      "metadata": {
        "id": "UKq1PsKkmeNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import linear regression model estimator from scikit-learn and instantiate**"
      ],
      "metadata": {
        "id": "6IB6KCQHpOAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression  # Imports the LinearRegression class from scikit-learn for linear regression modeling\n",
        "from sklearn import metrics                        # Imports the metrics module from scikit-learn for evaluating model performance\n"
      ],
      "metadata": {
        "id": "JyK4u-2YoptD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = LinearRegression() # Creating a Linear Regression object 'lm'"
      ],
      "metadata": {
        "id": "8w1OqhbypI-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm.fit(X_train,y_train) # Fit the linear model on to the 'lm' object itself i.e. no need to set this to another variable"
      ],
      "metadata": {
        "id": "Sm8uV42npf_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the intercept and coefficients and put them in a DataFrame**"
      ],
      "metadata": {
        "id": "EDazy5Vfpum_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The intercept term of the linear model:\", lm.intercept_)  # Prints the intercept term of the linear regression model 'lm'\n"
      ],
      "metadata": {
        "id": "oZ2fa67TpolS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The coefficients of the linear model:\", lm.coef_)  # Prints the coefficients of the linear regression model 'lm'\n"
      ],
      "metadata": {
        "id": "ayNe_p6Tp8Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf = pd.DataFrame(data=lm.coef_, index=X_train.columns, columns=[\"Coefficients\"])\n",
        "# Creating a DataFrame for the coefficients of the linear model, with feature names as the index and \"Coefficients\" as the column name\n",
        "cdf  # Displaying the DataFrame with the coefficients of the linear model"
      ],
      "metadata": {
        "id": "7qL6L69VqFhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculation of standard errors and t-statistic for the coefficients**"
      ],
      "metadata": {
        "id": "DwCKw0Upra7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = X_train.shape[0]  # Number of training samples\n",
        "k = X_train.shape[1]  # Number of features\n",
        "dfN = n - k           # Degrees of freedom for the residual error\n",
        "\n",
        "train_pred = lm.predict(X_train)  # Predicting values using the training feature set\n",
        "train_error = np.square(train_pred - y_train)  # Calculating squared errors between predicted and actual values\n",
        "sum_error = np.sum(train_error)  # Summing all squared errors\n",
        "\n",
        "se = [0, 0, 0, 0, 0]  # Initialize a list to store standard errors for each feature\n",
        "\n",
        "for i in range(k):\n",
        "    r = (sum_error / dfN)  # Calculating the mean squared error for the feature\n",
        "    r = r / np.sum(np.square(X_train[list(X_train.columns)[i]] - X_train[list(X_train.columns)[i]].mean()))\n",
        "    # Dividing by the variance of the feature\n",
        "    se[i] = np.sqrt(r)  # Calculating the standard error for the feature\n",
        "\n",
        "cdf['Standard Error'] = se  # Adding the standard errors to the DataFrame\n",
        "cdf['t-statistic'] = cdf['Coefficients'] / cdf['Standard Error']  # Calculating the t-statistic for each coefficient\n",
        "\n",
        "cdf  # Displaying the DataFrame with coefficients, standard errors, and t-statistics\n"
      ],
      "metadata": {
        "id": "1S9s3mS0q8DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Therefore, features arranged in the order of importance for predicting the house price\\n\", '-'*90, sep='')\n",
        "# Prints a header indicating that features are being arranged by importance, followed by a line of dashes for separation\n",
        "\n",
        "l = list(cdf.sort_values('t-statistic', ascending=False).index)\n",
        "# Sorts the features by their t-statistic in descending order and extracts the feature names into a list\n",
        "\n",
        "print(' > \\n'.join(l))\n",
        "# Prints the list of feature names, each separated by ' > ' and a newline\n"
      ],
      "metadata": {
        "id": "blalFcCYqd71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = list(cdf.index)  # Extracts the feature names from the DataFrame index into a list\n",
        "\n",
        "from matplotlib import gridspec  # Imports gridspec for creating complex grid layouts in matplotlib\n",
        "fig = plt.figure(figsize=(18, 10))  # Creates a new figure with a size of 18x10 inches\n",
        "gs = gridspec.GridSpec(2, 3)  # Defines a grid layout with 2 rows and 3 columns for subplots\n",
        "\n",
        "# Creating subplots within the defined grid layout\n",
        "\n",
        "ax0 = plt.subplot(gs[0])\n",
        "ax0.scatter(df[l[0]], df['Price'])  # Plots a scatter plot of the first feature against 'Price'\n",
        "ax0.set_title(l[0] + \" vs. Price\", fontdict={'fontsize': 20})  # Sets the title of the subplot\n",
        "\n",
        "ax1 = plt.subplot(gs[1])\n",
        "ax1.scatter(df[l[1]], df['Price'])  # Plots a scatter plot of the second feature against 'Price'\n",
        "ax1.set_title(l[1] + \" vs. Price\", fontdict={'fontsize': 20})  # Sets the title of the subplot\n",
        "\n",
        "ax2 = plt.subplot(gs[2])\n",
        "ax2.scatter(df[l[2]], df['Price'])  # Plots a scatter plot of the third feature against 'Price'\n",
        "ax2.set_title(l[2] + \" vs. Price\", fontdict={'fontsize': 20})  # Sets the title of the subplot\n",
        "\n",
        "ax3 = plt.subplot(gs[3])\n",
        "ax3.scatter(df[l[3]], df['Price'])  # Plots a scatter plot of the fourth feature against 'Price'\n",
        "ax3.set_title(l[3] + \" vs. Price\", fontdict={'fontsize': 20})  # Sets the title of the subplot\n",
        "\n",
        "ax4 = plt.subplot(gs[4])\n",
        "ax4.scatter(df[l[4]], df['Price'])  # Plots a scatter plot of the fifth feature against 'Price'\n",
        "ax4.set_title(l[4] + \" vs. Price\", fontdict={'fontsize': 20})  # Sets the title of the subplot\n"
      ],
      "metadata": {
        "id": "zbDNhEg4r2fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**R-square of the model fit**"
      ],
      "metadata": {
        "id": "ar_Fhm4ZsU9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"R-squared value of this fit:\", round(metrics.r2_score(y_train, train_pred), 3))\n",
        "# Calculates and prints the R-squared value (coefficient of determination) for the model's fit on the training data\n",
        "# 'metrics.r2_score' computes the R-squared value, and 'round(..., 3)' rounds the result to three decimal places\n"
      ],
      "metadata": {
        "id": "1YxNMxpvsCla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction, error estimate, and regression evaluation matrices\n",
        "\n",
        "**Prediction using the lm model**"
      ],
      "metadata": {
        "id": "tVyBWKpkslXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lm.predict(X_test)  # Predicts the target values for the test feature set (X_test) using the trained linear model\n",
        "\n",
        "print(\"Type of the predicted object:\", type(predictions))\n",
        "# Prints the type of the 'predictions' object to show it is a NumPy array or similar\n",
        "\n",
        "print(\"Size of the predicted object:\", predictions.shape)\n",
        "# Prints the size (shape) of the 'predictions' object to show the number of predicted values\n"
      ],
      "metadata": {
        "id": "GBzbPob3seZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scatter plot of predicted price and y_test set to see if the data fall on a 45 degree straight line**\n",
        "\n"
      ],
      "metadata": {
        "id": "T8XCcwBds6Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))  # Creates a new figure with a size of 10x7 inches\n",
        "plt.title(\"Actual vs. predicted house prices\", fontsize=25)  # Sets the title of the plot with a font size of 25\n",
        "plt.xlabel(\"Actual test set house prices\", fontsize=18)  # Labels the x-axis with a font size of 18\n",
        "plt.ylabel(\"Predicted house prices\", fontsize=18)  # Labels the y-axis with a font size of 18\n",
        "plt.scatter(x=y_test, y=predictions)  # Creates a scatter plot of actual test set house prices (x) vs. predicted house prices (y)\n"
      ],
      "metadata": {
        "id": "QMlanJqGsvyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting histogram of the residuals i.e. predicted errors (expect a normally distributed pattern)**"
      ],
      "metadata": {
        "id": "E-yUZ7netJM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))  # Creates a new figure with a size of 10x7 inches\n",
        "plt.title(\"Histogram of residuals to check for normality\", fontsize=25)  # Sets the title of the plot with a font size of 25\n",
        "plt.xlabel(\"Residuals\", fontsize=18)  # Labels the x-axis with a font size of 18\n",
        "plt.ylabel(\"Kernel density\", fontsize=18)  # Labels the y-axis with a font size of 18\n",
        "sns.histplot([y_test - predictions], kde=True)  # Creates a histogram of the residuals (differences between actual and predicted values)\n",
        "                                              # 'kde=True' adds a Kernel Density Estimate (KDE) to visualize the distribution more smoothly\n"
      ],
      "metadata": {
        "id": "WBV_o6f7tCWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scatter plot of residuals and predicted values (Homoscedasticity)**\n",
        "\n",
        "Homoscedasticity is a key assumption in linear regression analysis that means the variance of the error term is constant across all values of the independent variables. This means that the error term, or \"noise\", in the relationship between the independent and dependent variables, does not vary much as the value of the predictor variable changes\n",
        "\n"
      ],
      "metadata": {
        "id": "e_d_y9Zmtm-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))  # Creates a new figure with a size of 10x7 inches\n",
        "plt.title(\"Residuals vs. predicted values plot (Homoscedasticity)\\n\", fontsize=25)\n",
        "# Sets the title of the plot with a font size of 25 and adds a newline for separation\n",
        "plt.xlabel(\"Predicted house prices\", fontsize=18)  # Labels the x-axis with a font size of 18\n",
        "plt.ylabel(\"Residuals\", fontsize=18)  # Labels the y-axis with a font size of 18\n",
        "plt.scatter(x=predictions, y=y_test - predictions)\n",
        "# Creates a scatter plot of predicted house prices (x) vs. residuals (differences between actual and predicted values)\n"
      ],
      "metadata": {
        "id": "iqRTwyKWtrKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression evaluation metrices**"
      ],
      "metadata": {
        "id": "2lPXgFhvuZMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean absolute error (MAE):\", metrics.mean_absolute_error(y_test, predictions))\n",
        "# Calculates and prints the Mean Absolute Error (MAE) between the actual and predicted values for the test set\n",
        "\n",
        "print(\"Mean square error (MSE):\", metrics.mean_squared_error(y_test, predictions))\n",
        "# Calculates and prints the Mean Squared Error (MSE) between the actual and predicted values for the test set\n",
        "\n",
        "print(\"Root mean square error (RMSE):\", np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
        "# Calculates and prints the Root Mean Squared Error (RMSE) between the actual and predicted values for the test set\n",
        "# RMSE is the square root of the MSE\n"
      ],
      "metadata": {
        "id": "_kBd5cPTty5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**R-square value**"
      ],
      "metadata": {
        "id": "Ic1ttAnmure7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"R-squared value of predictions:\", round(metrics.r2_score(y_test, predictions), 3))\n",
        "# Calculates and prints the R-squared value (coefficient of determination) for the model's predictions on the test set\n",
        "# 'metrics.r2_score' computes the R-squared value, and 'round(..., 3)' rounds the result to three decimal places\n"
      ],
      "metadata": {
        "id": "QTr01nlyuiox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute minmax value for observed price and expected price\n",
        "import numpy as np\n",
        "min=np.min(predictions/6000)\n",
        "max=np.max(predictions/12000)\n",
        "print(min, max)"
      ],
      "metadata": {
        "id": "_Vg-XQMlvA38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute MinMax value for Price=100\n",
        "L = (100 - min) / (max - min)\n",
        "# Calculates the Min-Max normalization value for a house price of 100, where 'min' and 'max' are the minimum and maximum values of the 'Price' variable\n",
        "\n",
        "L  # Displays the normalized value for Price=100\n",
        "\n",
        "plt.hist(L)  # Creates a histogram of the Min-Max normalized values\n"
      ],
      "metadata": {
        "id": "IgjPo591vRbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ao-1NbwVvTcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}